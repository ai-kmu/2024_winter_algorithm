{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_O2ZtuXzj00b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time\n",
        "from copy import deepcopy # Add Deepcopy for args\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
        "filename = \"cifar-100-python.tar.gz\"\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "# x: extract: 압축 풀기 (c: 압축하기), v: view, 과정 자세하게 보기, z:압축 f: 파일\n",
        "#이를 운영체제 명령어로 바꿔 운영체제에게 전달\n",
        "os.system(\"tar xvzf \" + filename)\n",
        "\n",
        "\n",
        "def unpickle(file):\n",
        "  # with 벗어나면 알아서 파일이 닫히게끔 함\n",
        "  # read binary(이진법으로 읽기)\n",
        "    with open(file, 'rb') as f:\n",
        "        dict = pickle.load(f, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "# 메타데이터\n",
        "meta = unpickle('cifar-100-python/meta')\n",
        "fine_label_names = [t.decode('utf8') for t in meta[b'fine_label_names']]\n",
        "\n",
        "#데이터셋\n",
        "data = unpickle('cifar-100-python/train')\n",
        "test = unpickle('cifar-100-python/test')\n",
        "\n",
        "#6:2로 트레인 테스트 나누기\n",
        "x_train, x_val, y_train, y_val = train_test_split(data[b'data'], data[b'fine_labels'], test_size=0.25, random_state=41)\n",
        "#테스트셋\n",
        "x_test = test[b'data']\n",
        "y_test = test[b'fine_labels']\n",
        "\n",
        "# #높이 너비 채널 순\n",
        "# def reshape_image(image):\n",
        "#     return np.transpose(np.reshape(image,(3, 32,32)), (1,2,0))"
      ],
      "metadata": {
        "id": "Qqm2SMvWj45D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "def cifar100_dataset(data, labels, transform=None):\n",
        "    transformed_data = []\n",
        "    for image, label in zip(data, labels):\n",
        "        # 이미지 데이터를 (높이, 너비, 채널 수) 형태로 변환\n",
        "        image = image.reshape(3, 32, 32).transpose((1, 2, 0))\n",
        "        if transform:\n",
        "            image = transform(image)\n",
        "        transformed_data.append((image, label))\n",
        "    return transformed_data\n",
        "\n",
        "# 이미지 데이터를 PyTorch tensor로 변환하고 정규화하는 변환\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "transform_train1 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomHorizontalFlip(),           # 무작위로 좌우 반전\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_train2 = transforms.Compose([\n",
        "    transforms.ToTensor(),                       # 이미지를 PyTorch Tensor로 변환\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color Jittering\n",
        "    transforms.GaussianBlur(kernel_size=3),  # Random Gaussian Blur\n",
        "    transforms.RandomHorizontalFlip(),           # 무작위로 좌우 반전\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 이미지 정규화\n",
        "])\n",
        "\n",
        "# 훈련 데이터와 검증 데이터 변환\n",
        "trainset1 = cifar100_dataset(np.array(x_train), y_train, transform_train1)\n",
        "trainset2 = cifar100_dataset(np.array(x_train), y_train, transform_train2)\n",
        "combined_trainset = ConcatDataset([trainset1, trainset2])\n",
        "\n",
        "valset1 = cifar100_dataset(np.array(x_val), y_val, transform_train1)\n",
        "valset2 = cifar100_dataset(np.array(x_val), y_val, transform_train2)\n",
        "combined_valset = ConcatDataset([valset1, valset2])\n",
        "\n",
        "testset = cifar100_dataset(np.array(x_test),y_test, transform_test)\n",
        "\n",
        "# 훈련 세션에 사용할 데이터 파티션\n",
        "partition = {'train': combined_trainset, 'val': combined_valset, 'test':testset}"
      ],
      "metadata": {
        "id": "GR6YL3nubyIF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv1x1(in_planes, out_planes, stride=1):#1x1 컨볼루젼\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):#3x3 컨볼루젼\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        self.inplanes = inplanes\n",
        "        self.planes = planes\n",
        "\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        identity = x\n",
        "\n",
        "        out = self.relu(self.bn1(self.conv1(x)))#컨배렐\n",
        "        out = self.bn2(self.conv2(out))#컨배\n",
        "        if self.downsample is not None:# conv통과하면서 사이즈 작아지면 레지듀얼 브랜치 다운샘플링(크기 맞춰주기)\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity #레지듀얼브랜치값+ 컨배렐컨배 결과값\n",
        "        out = self.relu(out) #비선형함수 렐루에 넣기(결국 컨배렐컨배렐 순서임 ※렐루 전에 배치놂 해야함)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "8mAr8K37j6rS"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, layers, num_classes=100, zero_init_residual=False):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.inplanes = 64\n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    self.layer1 = self._make_layer(block, 128, layers[0])\n",
        "    self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "    self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
        "\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    # Zero-initialize the last BN in each residual branch,\n",
        "    # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "    if zero_init_residual:\n",
        "      for m in self.modules():\n",
        "        if isinstance(m, Bottleneck):\n",
        "          nn.init.constant_(m.bn3.weight, 0)\n",
        "        elif isinstance(m, BasicBlock):\n",
        "          nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "\n",
        "  def _make_layer(self, block, planes, blocks, stride=1):\n",
        "    downsample = None\n",
        "    if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "      downsample = nn.Sequential(\n",
        "          conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "          nn.BatchNorm2d(planes * block.expansion),\n",
        "      )\n",
        "\n",
        "    layers = []\n",
        "    layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "    self.inplanes = planes * block.expansion\n",
        "    for _ in range(1, blocks):\n",
        "      layers.append(block(self.inplanes, planes))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "2gyx8IWLj761"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, partition, optimizer, criterion, args):\n",
        "  trainloader = torch.utils.data.DataLoader(partition['train'],\n",
        "                                            batch_size=args.train_batch_size, shuffle=True, num_workers=2)\n",
        "  net.train()\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  train_loss = 0.0\n",
        "\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "    optimizer.zero_grad()\n",
        "    # get the inputs\n",
        "    inputs, labels = data\n",
        "    inputs = inputs.cuda()\n",
        "    labels = labels.cuda()\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "  train_loss = train_loss / len(trainloader)\n",
        "  train_acc = 100 * correct / total\n",
        "\n",
        "  return net, train_loss, train_acc\n"
      ],
      "metadata": {
        "id": "-oT3k2h4kd56"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(net, partition, criterion, args):\n",
        "  valloader = torch.utils.data.DataLoader(partition['val'],\n",
        "                                           batch_size=args.test_batch_size, shuffle=False, num_workers=2)\n",
        "  net.eval()\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  val_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in valloader:\n",
        "      images, labels = data\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      outputs = net(images)\n",
        "\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      val_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss = val_loss / len(valloader)\n",
        "    val_acc = 100 * correct / total\n",
        "\n",
        "  return val_loss, val_acc\n"
      ],
      "metadata": {
        "id": "-lVoKfCHklxu"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(net, partition, args):\n",
        "  testloader = torch.utils.data.DataLoader(partition['test'],\n",
        "                                            batch_size=args.test_batch_size, shuffle=False, num_workers=2)\n",
        "  net.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in testloader:\n",
        "      images, labels = data\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      outputs = net(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_acc = 100 * correct / total\n",
        "  return test_acc"
      ],
      "metadata": {
        "id": "hs6ZUiahknT6"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment(partition, args):\n",
        "  net = ResNet(BasicBlock, [2,2,2,2])\n",
        "  net.cuda()\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  if args.optim == 'SGD':\n",
        "    optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "  elif args.optim == 'RMSprop':\n",
        "    optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "  elif args.optim == 'Adam':\n",
        "    optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "  else:\n",
        "    raise ValueError('In-valid optimizer choice')\n",
        "\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  train_accs = []\n",
        "  val_accs = []\n",
        "\n",
        "  for epoch in range(args.epoch): # loop over the dataset multiple times\n",
        "    ts = time.time()\n",
        "    net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
        "    val_loss, val_acc = validate(net, partition, criterion, args)\n",
        "    te = time.time()\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
        "\n",
        "  test_acc = test(net, partition, args)\n",
        "  print(test_acc, '!!!!!!!')\n",
        "  result = {}\n",
        "  result['train_losses'] = train_losses\n",
        "  result['val_losses'] = val_losses\n",
        "  result['train_accs'] = train_accs\n",
        "  result['val_accs'] = val_accs\n",
        "  result['train_acc'] = train_acc\n",
        "  result['val_acc'] = val_acc\n",
        "  result['test_acc'] = test_acc\n",
        "\n",
        "  return vars(args), result, args.l2, args.lr, args.epoch, test_acc\n"
      ],
      "metadata": {
        "id": "3miW_NoFkok1"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "def save_exp_result(setting, result, l2, lr, epoch):\n",
        "  exp_name = setting['exp_name']\n",
        "\n",
        "  hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
        "  filename = './l2:{}-lr:{}-epoch:{}-{}.json'.format(l2,lr, epoch, hash_key)\n",
        "  result.update(setting)\n",
        "\n",
        "  with open(filename, 'w') as f:\n",
        "    json.dump(result, f)\n",
        "\n",
        "def load_exp_result(exp_name):\n",
        "  dir_path = './results'\n",
        "  filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
        "  list_result = []\n",
        "\n",
        "  for filename in filenames:\n",
        "    if exp_name in filename:\n",
        "      with open(join(dir_path, filename), 'r') as infile:\n",
        "        results = json.load(infile)\n",
        "        list_result.append(results)\n",
        "\n",
        "  df = pd.DataFrame(list_result) # .drop(columns=[])\n",
        "  return df\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "args.exp_name = \"exp1_lr\"\n",
        "\n",
        "# ====== Model Capacity ====== #\n",
        "args.act = 'relu'\n",
        "\n",
        "# ====== Regularization ======= #\n",
        "args.l2 = 0.013\n",
        "\n",
        "# ====== Optimizer & Training ====== #\n",
        "args.optim = 'Adam' #'RMSprop' #SGD, RMSprop, ADAM...\n",
        "\n",
        "args.lr = 0.00016\n",
        "args.epoch = 80\n",
        "\n",
        "args.train_batch_size = 64\n",
        "args.test_batch_size = 1024\n",
        "\n",
        "setting, result, l2, lr, epoch, res = experiment(partition, args)"
      ],
      "metadata": {
        "id": "j2jkOgLbkpyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "159ad996-1b39-4fde-8468-6f2d08cfe217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Acc(train/val): 12.71/22.12, Loss(train/val) 3.84/3.26. Took 16.82 sec\n",
            "Epoch 1, Acc(train/val): 25.35/28.26, Loss(train/val) 3.08/2.94. Took 17.44 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_exp_result(setting, result, l2, lr, epoch, res):\n",
        "  exp_name = setting['exp_name']\n",
        "\n",
        "  hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
        "  filename = './{}-l2:{}-lr:{}-epoch:{}-{}.json'.format(res, l2,lr, epoch, hash_key)\n",
        "  result.update(setting)\n",
        "\n",
        "  with open(filename, 'w') as f:\n",
        "    json.dump(result, f)\n",
        "\n",
        "save_exp_result(setting, result, l2, lr, epoch, res)"
      ],
      "metadata": {
        "id": "9F55ST-u2R3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xWvcodRr2SMP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}