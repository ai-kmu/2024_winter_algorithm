{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn # torch.nn 클래스와 함수\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim # 다양한 optimization\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time\n",
        "from copy import deepcopy\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request # 파일 다운로드\n",
        "import tarfile # 파일 추출\n",
        "from sklearn.model_selection import train_test_split # 무작위로 데이터 나누기\n",
        "import os\n",
        "import pickle"
      ],
      "metadata": {
        "id": "vfzBbhnVyAA-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tkkfmDk7x4MN"
      },
      "outputs": [],
      "source": [
        "url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
        "path = \"./cifar-100.tar.gz\" # 다운로드할 경로\n",
        "urllib.request.urlretrieve(url, path) # 데이터셋 다운\n",
        "with tarfile.open(path, 'r:gz') as tar: # 압축파일 열기\n",
        "    tar.extractall() # 압축 해제"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # 텐서로 이미지 변환\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # 평균, 표준편차 0.5로 각 채널 정규화\n",
        "])\n",
        "\n",
        "# meta data (클래스 정보)\n",
        "with open('cifar-100-python/meta', 'rb') as fo: # 이진모드로 읽음\n",
        "    meta = pickle.load(fo, encoding=\"bytes\") # byte 문자열 디코딩\n",
        "\n",
        "# train data (사진 정보)\n",
        "with open('cifar-100-python/train', 'rb') as fo:\n",
        "    train = pickle.load(fo, encoding=\"bytes\")\n",
        "\n",
        "# 하위 레이블 이름 디코딩 -> 리스트로 변환\n",
        "fine_label_names = [label.decode('utf8') for label in meta[b'fine_label_names']]\n",
        "\n",
        "# 함수를 사용하여 데이터셋 분할\n",
        "train_img, val_img, train_label, val_label = train_test_split(train[b'data'], train[b'fine_labels'], test_size=0.2, random_state=30)\n",
        "\n",
        "# 데이터셋 정의\n",
        "class MakeDataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx): # 인덱스에 해당하는 이미지 데이터, 레이블 반환\n",
        "        image = self.data[idx].reshape(3, 32, 32).transpose(1, 2, 0)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# 데이터셋 생성\n",
        "trainset = MakeDataSet(train_img, train_label, transform=transform)\n",
        "valset = MakeDataSet(val_img, val_label, transform=transform)\n",
        "\n",
        "# 파티션 생성\n",
        "partition = {'train': trainset, 'val': valset}"
      ],
      "metadata": {
        "id": "Dp_vzOG17H47"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG 네트워크 아키텍처 정의\n",
        "# 64, 128.. 개의 필터를 가진 conv layer\n",
        "# MaxPooling\n",
        "\n",
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}"
      ],
      "metadata": {
        "id": "UnVOGI7J71nP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "    # 생성자 메서드\n",
        "    def __init__(self, model_code, in_channels, out_dim, act, use_bn):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # activation func\n",
        "        if act == 'relu':\n",
        "            self.act = nn.ReLU()\n",
        "        elif act == 'sigmoid':\n",
        "            self.act = nn.Sigmoid()\n",
        "        elif act == 'tanh':\n",
        "            self.act = nn.TanH()\n",
        "        else:\n",
        "            raise ValueError(\"Not a valid activation function code\")\n",
        "\n",
        "        # layer 생성\n",
        "        self.layers = self._make_layers(model_code, in_channels, use_bn)\n",
        "        # classifier 생성, 여기서는 fullyconnected\n",
        "        self.classifer = nn.Sequential(nn.Linear(512, 256),\n",
        "                                       self.act,\n",
        "                                       nn.Linear(256, out_dim))\n",
        "    # forward 함수\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifer(x)\n",
        "        return x\n",
        "\n",
        "    # 컨볼루션 레이어 생성\n",
        "    def _make_layers(self, model_code, in_channels, use_bn):\n",
        "        layers = []\n",
        "        for x in cfg[model_code]:\n",
        "            # M 일때는 MaxPooling\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            # 숫자일 때는 conv layer\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels=in_channels,\n",
        "                                     out_channels=x,\n",
        "                                     kernel_size=3,\n",
        "                                     stride=1,\n",
        "                                     padding=1)]\n",
        "                # batch norm 사용\n",
        "                if use_bn:\n",
        "                    layers += [nn.BatchNorm2d(x)]\n",
        "                layers += [self.act]\n",
        "                in_channels = x\n",
        "\n",
        "        # 생성된 레이어 return\n",
        "        return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "blx_1eub1diP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, partition, optimizer, criterion, args):\n",
        "\n",
        "    # 미니배치로 나누기, 데이터 섞기, 병렬 처리\n",
        "    trainloader = torch.utils.data.DataLoader(partition['train'],\n",
        "                                              batch_size=args.train_batch_size,\n",
        "                                              shuffle=True, num_workers=2)\n",
        "\n",
        "    # train 모드\n",
        "    net.train()\n",
        "\n",
        "    # 변수 초기화\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    train_loss = 0.0\n",
        "\n",
        "    # 미니배치 순회\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 데이터 가져오기 및 GPU로 이동\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # loss, gradient 계산 및 파라미터 업데이트\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # train loss 축적\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1) # 예측된 클래스 가져오기\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item() # 맞은 샘플 수 업데이트\n",
        "\n",
        "    train_loss = train_loss / len(trainloader) # 전체 train loss 미니배치 개수로 나눔\n",
        "    train_acc = 100 * correct / total # 정확도 계산\n",
        "    return net, train_loss, train_acc\n"
      ],
      "metadata": {
        "id": "R1TL1Y4q7pkp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(net, partition, criterion, args):\n",
        "    valloader = torch.utils.data.DataLoader(partition['val'],\n",
        "                                            batch_size=args.test_batch_size,\n",
        "                                            shuffle=False, num_workers=2)\n",
        "    # test 모드\n",
        "    net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0\n",
        "    with torch.no_grad(): # gradient 추적 안 함\n",
        "        # 미니배치 순회\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = criterion(outputs, labels) # output, label간 loss 계산\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(valloader) # val 데이터셋에서의 loss\n",
        "        val_acc = 100 * correct / total # 정확도\n",
        "    return val_loss, val_acc\n"
      ],
      "metadata": {
        "id": "fbwKvdzw8F9e"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train 및 test 수행\n",
        "def experiment(partition, args):\n",
        "\n",
        "    # model 생성\n",
        "    net = CNN(model_code = args.model_code,\n",
        "              in_channels = args.in_channels,\n",
        "              out_dim = args.out_dim,\n",
        "              act = args.act,\n",
        "              use_bn = args.use_bn)\n",
        "    # GPU이동\n",
        "    net.cuda()\n",
        "\n",
        "    # loss함수로 crossEntropy사용\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # optimizer 선택\n",
        "    if args.optim == 'SGD':\n",
        "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2) # learning rate 및 norm\n",
        "    elif args.optim == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'Adam':\n",
        "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    else:\n",
        "        raise ValueError('In-valid optimizer choice')\n",
        "\n",
        "    # 중간 내용 저장할 리스트\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    # 주어진 에폭 수 만큼 돌기\n",
        "    for epoch in range(args.epoch):\n",
        "        ts = time.time() # 평가 시간 측정\n",
        "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
        "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
        "        te = time.time()\n",
        "\n",
        "        # list에 결과 저장\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
        "\n",
        "    # 모델의 weight 저장\n",
        "    torch.save(net.state_dict(), 'model_weights.pth')\n",
        "\n",
        "    # 결과 dic에 저장\n",
        "    result = {}\n",
        "    result['train_losses'] = train_losses\n",
        "    result['val_losses'] = val_losses\n",
        "    result['train_accs'] = train_accs\n",
        "    result['val_accs'] = val_accs\n",
        "    result['train_acc'] = train_acc\n",
        "    result['val_acc'] = val_acc\n",
        "\n",
        "    return vars(args), result\n"
      ],
      "metadata": {
        "id": "RFW3hTzf8MfI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Random Seed Initialization ====== #\n",
        "seed = 123 # 재현성 위해 랜덤 시드 초기화\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser() # 파서 생성\n",
        "args = parser.parse_args(\"\")\n",
        "# args.exp_name = \"exp1_lr_model_code\"\n",
        "\n",
        "# ====== Model ====== #\n",
        "args.model_code = 'VGG11' # 모델 코드\n",
        "args.in_channels = 3 # 컬러 이미지\n",
        "args.out_dim = 100 # 100개의 클래스\n",
        "args.act = 'relu'\n",
        "\n",
        "# ====== Regularization ======= #\n",
        "args.l2 = 0.0001 # L2 정규화\n",
        "args.use_bn = True # 배치 정규화\n",
        "\n",
        "# ====== Optimizer & Training ====== #\n",
        "args.optim = 'Adam' #'RMSprop' #SGD, RMSprop, ADAM...\n",
        "args.lr = 0.0015 # learning rate\n",
        "args.epoch = 15 # 에폭\n",
        "\n",
        "args.train_batch_size = 256 # 배치 크기\n",
        "args.test_batch_size = 1024\n",
        "\n",
        "# ====== Experiment Variable ====== #\n",
        "# name_var1 = 'lr'\n",
        "# name_var2 = 'model_code'\n",
        "# list_var1 = [0.0001, 0.00001]\n",
        "# list_var2 = ['VGG11', 'VGG13']\n",
        "\n",
        "\n",
        "# for var1 in list_var1:\n",
        "#     for var2 in list_var2:\n",
        "#         setattr(args, name_var1, var1)\n",
        "#         setattr(args, name_var2, var2)\n",
        "#         print(args)\n",
        "\n",
        "setting, result = experiment(partition, deepcopy(args)) # 실험 수행"
      ],
      "metadata": {
        "id": "M7ftN2LS8tBt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6b38b99-910b-4ba5-8d05-c1a34767320a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Acc(train/val): 3.02/5.24, Loss(train/val) 4.32/4.09. Took 10.09 sec\n",
            "Epoch 1, Acc(train/val): 8.47/9.69, Loss(train/val) 3.77/3.77. Took 10.49 sec\n",
            "Epoch 2, Acc(train/val): 15.36/17.83, Loss(train/val) 3.36/3.24. Took 10.45 sec\n",
            "Epoch 3, Acc(train/val): 22.69/15.89, Loss(train/val) 2.97/3.59. Took 10.32 sec\n",
            "Epoch 4, Acc(train/val): 29.22/28.25, Loss(train/val) 2.65/2.74. Took 10.30 sec\n",
            "Epoch 5, Acc(train/val): 35.27/32.10, Loss(train/val) 2.36/2.66. Took 10.71 sec\n",
            "Epoch 6, Acc(train/val): 41.07/36.41, Loss(train/val) 2.11/2.41. Took 10.17 sec\n",
            "Epoch 7, Acc(train/val): 46.60/36.92, Loss(train/val) 1.88/2.47. Took 10.57 sec\n",
            "Epoch 8, Acc(train/val): 51.29/38.85, Loss(train/val) 1.68/2.37. Took 10.61 sec\n",
            "Epoch 9, Acc(train/val): 56.65/35.97, Loss(train/val) 1.46/2.83. Took 10.54 sec\n",
            "Epoch 10, Acc(train/val): 61.74/41.69, Loss(train/val) 1.26/2.39. Took 10.59 sec\n",
            "Epoch 11, Acc(train/val): 67.03/42.98, Loss(train/val) 1.06/2.49. Took 10.85 sec\n",
            "Epoch 12, Acc(train/val): 72.54/43.01, Loss(train/val) 0.88/2.61. Took 10.41 sec\n",
            "Epoch 13, Acc(train/val): 77.22/43.10, Loss(train/val) 0.72/2.81. Took 10.46 sec\n",
            "Epoch 14, Acc(train/val): 80.58/44.02, Loss(train/val) 0.60/2.90. Took 10.44 sec\n"
          ]
        }
      ]
    }
  ]
}