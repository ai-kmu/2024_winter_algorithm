{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn # torch.nn 클래스와 함수\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim # 다양한 optimization\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time\n",
        "from copy import deepcopy\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request # 파일 다운로드\n",
        "import tarfile # 파일 추출\n",
        "from sklearn.model_selection import train_test_split # 무작위로 데이터 나누기\n",
        "import os\n",
        "import pickle"
      ],
      "metadata": {
        "id": "vfzBbhnVyAA-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkkfmDk7x4MN"
      },
      "outputs": [],
      "source": [
        "url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
        "path = \"./cifar-100.tar.gz\" # 다운로드할 경로\n",
        "urllib.request.urlretrieve(url, path) # 데이터셋 다운\n",
        "with tarfile.open(path, 'r:gz') as tar: # 압축파일 열기\n",
        "    tar.extractall() # 압축 해제"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # 텐서로 이미지 변환\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # 평균, 표준편차 0.5로 각 채널 정규화\n",
        "])\n",
        "\n",
        "# meta data (클래스 정보)\n",
        "with open('cifar-100-python/meta', 'rb') as fo: # 이진모드로 읽음\n",
        "    meta = pickle.load(fo, encoding=\"bytes\") # byte 문자열 디코딩\n",
        "\n",
        "# train data (사진 정보)\n",
        "with open('cifar-100-python/train', 'rb') as fo:\n",
        "    train = pickle.load(fo, encoding=\"bytes\")\n",
        "\n",
        "# 하위 레이블 이름 디코딩 -> 리스트로 변환\n",
        "fine_label_names = [label.decode('utf8') for label in meta[b'fine_label_names']]\n",
        "\n",
        "# 함수를 사용하여 데이터셋 분할\n",
        "train_img, val_img, train_label, val_label = train_test_split(train[b'data'], train[b'fine_labels'], test_size=0.2, random_state=30)\n",
        "\n",
        "# 데이터셋 정의\n",
        "class MakeDataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx): # 인덱스에 해당하는 이미지 데이터, 레이블 반환\n",
        "        image = self.data[idx].reshape(3, 32, 32).transpose(1, 2, 0)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# 데이터셋 생성\n",
        "trainset = MakeDataSet(train_img, train_label, transform=transform)\n",
        "valset = MakeDataSet(val_img, val_label, transform=transform)\n",
        "\n",
        "# 파티션 생성\n",
        "partition = {'train': trainset, 'val': valset}"
      ],
      "metadata": {
        "id": "Dp_vzOG17H47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG 네트워크 아키텍처 정의\n",
        "# 64, 128.. 개의 필터를 가진 conv layer\n",
        "# MaxPooling\n",
        "\n",
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}"
      ],
      "metadata": {
        "id": "UnVOGI7J71nP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "    # 생성자 메서드\n",
        "    def __init__(self, model_code, in_channels, out_dim, act, use_bn):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # activation func\n",
        "        if act == 'relu':\n",
        "            self.act = nn.ReLU()\n",
        "        elif act == 'sigmoid':\n",
        "            self.act = nn.Sigmoid()\n",
        "        elif act == 'tanh':\n",
        "            self.act = nn.TanH()\n",
        "        else:\n",
        "            raise ValueError(\"Not a valid activation function code\")\n",
        "\n",
        "        # layer 생성\n",
        "        self.layers = self._make_layers(model_code, in_channels, use_bn)\n",
        "        # classifier 생성, 여기서는 fullyconnected\n",
        "        self.classifer = nn.Sequential(nn.Linear(512, 256),\n",
        "                                       self.act,\n",
        "                                       nn.Linear(256, out_dim))\n",
        "    # forward 함수\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifer(x)\n",
        "        return x\n",
        "\n",
        "    # 컨볼루션 레이어 생성\n",
        "    def _make_layers(self, model_code, in_channels, use_bn):\n",
        "        layers = []\n",
        "        for x in cfg[model_code]:\n",
        "            # M 일때는 MaxPooling\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            # 숫자일 때는 conv layer\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels=in_channels,\n",
        "                                     out_channels=x,\n",
        "                                     kernel_size=3,\n",
        "                                     stride=1,\n",
        "                                     padding=1)]\n",
        "                # batch norm 사용\n",
        "                if use_bn:\n",
        "                    layers += [nn.BatchNorm2d(x)]\n",
        "                layers += [self.act]\n",
        "                in_channels = x\n",
        "\n",
        "        # 생성된 레이어 return\n",
        "        return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "blx_1eub1diP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, partition, optimizer, criterion, args):\n",
        "\n",
        "    # 미니배치로 나누기, 데이터 섞기, 병렬 처리\n",
        "    trainloader = torch.utils.data.DataLoader(partition['train'],\n",
        "                                              batch_size=args.train_batch_size,\n",
        "                                              shuffle=True, num_workers=2)\n",
        "\n",
        "    # train 모드\n",
        "    net.train()\n",
        "\n",
        "    # 변수 초기화\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    train_loss = 0.0\n",
        "\n",
        "    # 미니배치 순회\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 데이터 가져오기 및 GPU로 이동\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # loss, gradient 계산 및 파라미터 업데이트\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # train loss 축적\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1) # 예측된 클래스 가져오기\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item() # 맞은 샘플 수 업데이트\n",
        "\n",
        "    train_loss = train_loss / len(trainloader) # 전체 train loss 미니배치 개수로 나눔\n",
        "    train_acc = 100 * correct / total # 정확도 계산\n",
        "    return net, train_loss, train_acc\n"
      ],
      "metadata": {
        "id": "R1TL1Y4q7pkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(net, partition, criterion, args):\n",
        "    valloader = torch.utils.data.DataLoader(partition['val'],\n",
        "                                            batch_size=args.test_batch_size,\n",
        "                                            shuffle=False, num_workers=2)\n",
        "    # test 모드\n",
        "    net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0\n",
        "    with torch.no_grad(): # gradient 추적 안 함\n",
        "        # 미니배치 순회\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = criterion(outputs, labels) # output, label간 loss 계산\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(valloader) # val 데이터셋에서의 loss\n",
        "        val_acc = 100 * correct / total # 정확도\n",
        "    return val_loss, val_acc\n"
      ],
      "metadata": {
        "id": "fbwKvdzw8F9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train 및 test 수행\n",
        "def experiment(partition, args):\n",
        "\n",
        "    # model 생성\n",
        "    net = CNN(model_code = args.model_code,\n",
        "              in_channels = args.in_channels,\n",
        "              out_dim = args.out_dim,\n",
        "              act = args.act,\n",
        "              use_bn = args.use_bn)\n",
        "    # GPU이동\n",
        "    net.cuda()\n",
        "\n",
        "    # loss함수로 crossEntropy사용\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # optimizer 선택\n",
        "    if args.optim == 'SGD':\n",
        "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2) # learning rate 및 norm\n",
        "    elif args.optim == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'Adam':\n",
        "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    else:\n",
        "        raise ValueError('In-valid optimizer choice')\n",
        "\n",
        "    # 중간 내용 저장할 리스트\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    # 주어진 에폭 수 만큼 돌기\n",
        "    for epoch in range(args.epoch):\n",
        "        ts = time.time() # 평가 시간 측정\n",
        "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
        "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
        "        te = time.time()\n",
        "\n",
        "        # list에 결과 저장\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
        "\n",
        "    # 모델의 weight 저장\n",
        "    torch.save(net.state_dict(), 'model_weights.pth')\n",
        "\n",
        "    # 결과 dic에 저장\n",
        "    result = {}\n",
        "    result['train_losses'] = train_losses\n",
        "    result['val_losses'] = val_losses\n",
        "    result['train_accs'] = train_accs\n",
        "    result['val_accs'] = val_accs\n",
        "    result['train_acc'] = train_acc\n",
        "    result['val_acc'] = val_acc\n",
        "\n",
        "    return vars(args), result\n"
      ],
      "metadata": {
        "id": "RFW3hTzf8MfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Random Seed Initialization ====== #\n",
        "seed = 123 # 재현성 위해 랜덤 시드 초기화\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser() # 파서 생성\n",
        "args = parser.parse_args(\"\")\n",
        "# args.exp_name = \"exp1_lr_model_code\"\n",
        "\n",
        "# ====== Model ====== #\n",
        "args.model_code = 'VGG16' # 모델 코드\n",
        "args.in_channels = 3 # 컬러 이미지\n",
        "args.out_dim = 100 # 100개의 클래스\n",
        "args.act = 'relu'\n",
        "\n",
        "# ====== Regularization ======= #\n",
        "args.l2 = 0.0001 # L2 정규화\n",
        "args.use_bn = True # 배치 정규화\n",
        "\n",
        "# ====== Optimizer & Training ====== #\n",
        "args.optim = 'Adam' #'RMSprop' #SGD, RMSprop, ADAM...\n",
        "args.lr = 0.0015 # learning rate\n",
        "args.epoch = 40 # 에폭\n",
        "\n",
        "args.train_batch_size = 256 # 배치 크기\n",
        "args.test_batch_size = 1024\n",
        "\n",
        "# ====== Experiment Variable ====== #\n",
        "# name_var1 = 'lr'\n",
        "# name_var2 = 'model_code'\n",
        "# list_var1 = [0.0001, 0.00001]\n",
        "# list_var2 = ['VGG11', 'VGG13']\n",
        "\n",
        "\n",
        "# for var1 in list_var1:\n",
        "#     for var2 in list_var2:\n",
        "#         setattr(args, name_var1, var1)\n",
        "#         setattr(args, name_var2, var2)\n",
        "#         print(args)\n",
        "\n",
        "setting, result = experiment(partition, deepcopy(args)) # 실험 수행"
      ],
      "metadata": {
        "id": "M7ftN2LS8tBt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63e2db99-308a-47a9-efe3-ad6f55d5e6d5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Acc(train/val): 2.06/2.94, Loss(train/val) 4.43/4.27. Took 19.57 sec\n",
            "Epoch 1, Acc(train/val): 3.39/3.77, Loss(train/val) 4.17/4.27. Took 18.96 sec\n",
            "Epoch 2, Acc(train/val): 4.96/6.20, Loss(train/val) 4.00/3.93. Took 17.88 sec\n",
            "Epoch 3, Acc(train/val): 8.26/7.11, Loss(train/val) 3.74/3.93. Took 18.17 sec\n",
            "Epoch 4, Acc(train/val): 12.19/9.18, Loss(train/val) 3.44/3.84. Took 18.35 sec\n",
            "Epoch 5, Acc(train/val): 16.40/14.27, Loss(train/val) 3.19/3.44. Took 18.24 sec\n",
            "Epoch 6, Acc(train/val): 19.60/16.84, Loss(train/val) 2.99/3.24. Took 18.03 sec\n",
            "Epoch 7, Acc(train/val): 23.34/21.74, Loss(train/val) 2.81/3.05. Took 17.91 sec\n",
            "Epoch 8, Acc(train/val): 26.96/25.13, Loss(train/val) 2.62/2.79. Took 17.89 sec\n",
            "Epoch 9, Acc(train/val): 30.30/23.96, Loss(train/val) 2.47/2.91. Took 17.91 sec\n",
            "Epoch 10, Acc(train/val): 34.59/28.62, Loss(train/val) 2.28/2.83. Took 17.96 sec\n",
            "Epoch 11, Acc(train/val): 38.89/29.43, Loss(train/val) 2.12/2.74. Took 18.05 sec\n",
            "Epoch 12, Acc(train/val): 42.29/33.72, Loss(train/val) 1.96/2.54. Took 17.90 sec\n",
            "Epoch 13, Acc(train/val): 46.64/31.58, Loss(train/val) 1.79/2.76. Took 18.06 sec\n",
            "Epoch 14, Acc(train/val): 50.00/35.82, Loss(train/val) 1.64/2.64. Took 17.87 sec\n",
            "Epoch 15, Acc(train/val): 54.29/38.64, Loss(train/val) 1.49/2.55. Took 17.92 sec\n",
            "Epoch 16, Acc(train/val): 58.48/39.17, Loss(train/val) 1.33/2.62. Took 17.86 sec\n",
            "Epoch 17, Acc(train/val): 62.35/39.97, Loss(train/val) 1.19/2.73. Took 18.10 sec\n",
            "Epoch 18, Acc(train/val): 66.37/39.02, Loss(train/val) 1.06/2.84. Took 17.93 sec\n",
            "Epoch 19, Acc(train/val): 69.29/42.68, Loss(train/val) 0.95/2.62. Took 18.22 sec\n",
            "Epoch 20, Acc(train/val): 73.40/42.26, Loss(train/val) 0.81/2.94. Took 17.74 sec\n",
            "Epoch 21, Acc(train/val): 76.49/42.15, Loss(train/val) 0.72/3.16. Took 18.27 sec\n",
            "Epoch 22, Acc(train/val): 78.73/43.52, Loss(train/val) 0.65/2.95. Took 17.86 sec\n",
            "Epoch 23, Acc(train/val): 81.22/41.35, Loss(train/val) 0.56/3.27. Took 18.37 sec\n",
            "Epoch 24, Acc(train/val): 83.18/40.71, Loss(train/val) 0.51/3.64. Took 17.74 sec\n",
            "Epoch 25, Acc(train/val): 84.97/43.55, Loss(train/val) 0.45/3.43. Took 18.41 sec\n",
            "Epoch 26, Acc(train/val): 86.13/43.40, Loss(train/val) 0.42/3.60. Took 17.83 sec\n",
            "Epoch 27, Acc(train/val): 86.42/43.79, Loss(train/val) 0.42/3.58. Took 18.17 sec\n",
            "Epoch 28, Acc(train/val): 89.20/44.65, Loss(train/val) 0.33/3.79. Took 17.81 sec\n",
            "Epoch 29, Acc(train/val): 89.40/44.30, Loss(train/val) 0.32/3.86. Took 18.06 sec\n",
            "Epoch 30, Acc(train/val): 89.90/43.95, Loss(train/val) 0.31/3.88. Took 17.85 sec\n",
            "Epoch 31, Acc(train/val): 89.57/44.63, Loss(train/val) 0.31/3.81. Took 17.80 sec\n",
            "Epoch 32, Acc(train/val): 91.90/42.43, Loss(train/val) 0.25/4.39. Took 17.92 sec\n",
            "Epoch 33, Acc(train/val): 92.23/42.94, Loss(train/val) 0.24/4.25. Took 17.81 sec\n",
            "Epoch 34, Acc(train/val): 91.40/44.09, Loss(train/val) 0.27/4.01. Took 17.95 sec\n",
            "Epoch 35, Acc(train/val): 91.89/44.51, Loss(train/val) 0.26/4.17. Took 17.72 sec\n",
            "Epoch 36, Acc(train/val): 92.39/44.06, Loss(train/val) 0.24/4.04. Took 17.92 sec\n",
            "Epoch 37, Acc(train/val): 92.85/43.68, Loss(train/val) 0.22/4.30. Took 17.75 sec\n",
            "Epoch 38, Acc(train/val): 93.50/45.32, Loss(train/val) 0.20/4.07. Took 17.88 sec\n",
            "Epoch 39, Acc(train/val): 92.83/43.00, Loss(train/val) 0.22/4.29. Took 17.74 sec\n"
          ]
        }
      ]
    }
  ]
}