{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Yj3dhUnQP9ZE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time\n",
        "import pickle\n",
        "from copy import deepcopy # Add Deepcopy for args\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR-100 데이터셋 다운로드\n",
        "url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
        "filename = \"cifar-100-python.tar.gz\"\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "    urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "# 압축 풀기\n",
        "with tarfile.open(filename, 'r:gz') as tar:\n",
        "    tar.extractall()\n",
        "\n",
        "# 데이터 불러오기\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "# CustomCIFAR100 클래스 정의\n",
        "class CustomCIFAR100(Dataset):\n",
        "    def __init__(self, root, train=True, transform=None):\n",
        "        self.root = os.path.abspath(root)  # root를 절대 경로로 설정\n",
        "        self.train = train\n",
        "        self.transform = transform\n",
        "\n",
        "        data_file_name = \"train\" if self.train else \"test\"\n",
        "        data_file = os.path.join(self.root, \"cifar-100-python\", data_file_name)\n",
        "\n",
        "        with open(data_file, 'rb') as fo:\n",
        "            data_dict = pickle.load(fo, encoding='bytes')\n",
        "            self.data = data_dict[b'data']\n",
        "            self.fine_labels = data_dict[b'fine_labels']\n",
        "\n",
        "    #데이터셋의 전체 길이 반환\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    # 주어진 index에 대한 data smaple 반환\n",
        "    def __getitem__(self, idx):\n",
        "      image = self.data[idx]\n",
        "      label = self.fine_labels[idx]\n",
        "\n",
        "      # 이미지 차원 확인\n",
        "      if len(image.shape) == 1:\n",
        "\n",
        "          # 이미지 차원이 1인 경우, 예를 들어 (3072,)인 경우 (32, 32, 3)으로 변경\n",
        "          image = image.reshape(3, 32, 32).transpose(1, 2, 0)\n",
        "\n",
        "      #이미지 변환이 지정되어 있다면 해당 변환 적용\n",
        "      if self.transform:\n",
        "          image = self.transform(image)\n",
        "\n",
        "      return image, label\n",
        "\n",
        "\n",
        "# 이미지에 대한 변환을 정의\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 이미지를 [-1, 1] 범위로 정규화합니다.\n",
        "])\n",
        "\n",
        "# 메타데이터 불러오기\n",
        "meta = unpickle('cifar-100-python/meta')\n",
        "fine_label_names = [t.decode('utf8') for t in meta[b'fine_label_names']]\n",
        "\n",
        "# 훈련 데이터 불러오기\n",
        "train = unpickle('cifar-100-python/train')\n",
        "\n",
        "train_data = train[b'data']\n",
        "train_labels = train[b'fine_labels']\n",
        "\n",
        "# train과 validation data set 나누기 (8:2 비율)\n",
        "# test_size 0.2가 validation에 20%를 사용하겠다는 의미.\n",
        "# random_state : 데이터 분할 시 사용되는 무작위성을 제어\n",
        "x_train, x_val, y_train, y_val = train_test_split(train[b'data'], train[b'fine_labels'], test_size=0.2, random_state=50)\n",
        "\n",
        "# train dataset 정의\n",
        "train_dataset =  CustomCIFAR100(root='/content', train=True, transform=transform)\n",
        "\n",
        "# validation dataset 정의\n",
        "val_dataset = CustomCIFAR100(root='/content', train=False, transform=transform)\n",
        "\n",
        "\n",
        "# 훈련 세션에 사용할 데이터 partition\n",
        "partition = {'train': train_dataset, 'val': val_dataset}"
      ],
      "metadata": {
        "id": "3o9DJsS6QAY5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VGGNet을 정의하는 dictionary\n",
        "# 숫자: convolutional layer의 출력 채널 수, M: Max pooling layer\n",
        "cfg = {\n",
        "        'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']\n",
        "}"
      ],
      "metadata": {
        "id": "uIhI4mjhQdpu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self, model_code, in_channels, out_dim, act, use_bn=True):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # Activation 함수 선택\n",
        "        if act == 'relu':\n",
        "            self.act = nn.ReLU()\n",
        "        elif act == 'sigmoid':\n",
        "            self.act = nn.Sigmoid()\n",
        "        elif act == 'tanh':\n",
        "            self.act = nn.TanH()\n",
        "        else:\n",
        "            raise ValueError(\"Not a valid activation function code\")\n",
        "\n",
        "        # layers 생성하는 함수 호출\n",
        "        self.layers = self._make_layers(model_code, in_channels, use_bn)\n",
        "\n",
        "        # 분류를 위한 FC layer\n",
        "        self.classifier = nn.Sequential(nn.Linear(512, 256),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Linear(256, out_dim))\n",
        "\n",
        "    # 모델의 forward 연산 정의\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _make_layers(self, model_code, in_channels, use_bn):\n",
        "        layers = []\n",
        "        for x in cfg[model_code]:\n",
        "            if x == 'M':\n",
        "                # MaxPooling 레이어 생성\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                # Convolution layer 구성\n",
        "                layers += [nn.Conv2d(in_channels=in_channels,\n",
        "                                     out_channels=x,  # out_channels은 forward 연산 이후 x\n",
        "                                     kernel_size=3,   # CNN kernel_size\n",
        "                                     stride=1,        # CNN kernel의 stride\n",
        "                                     padding=1)]      # CNN kernel의 padding\n",
        "\n",
        "                # BN(Batch Normalization을 사용하는 경우 아래를 사용)\n",
        "                if use_bn:\n",
        "                    layers += [nn.BatchNorm2d(x)]\n",
        "                layers += [self.act]   # 레이어 이후 활성화 함수 적용\n",
        "                in_channels = x        # 다음 들어가야하는 레이어에 입력 채널 수 업데이트\n",
        "\n",
        "        return nn.Sequential(*layers)\n"
      ],
      "metadata": {
        "id": "_GJYSZrcQigQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, partition, optimizer, criterion, args):\n",
        "\n",
        "    #데이터 로더 설정\n",
        "    trainloader = torch.utils.data.DataLoader(partition['train'],\n",
        "                                              batch_size=args.train_batch_size,\n",
        "                                              shuffle=True, num_workers=2)\n",
        "    # 네트워크를 학습 모드로 설정\n",
        "    net.train()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    train_loss = 0.0\n",
        "\n",
        "    # DateLoader에서 미니배치 단위로 데이터를 가져와 학습 수행\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 미니배치에서 입력과 정답 가져오기\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        # Foward\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # 손실 함수 계산\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward 및 최적화\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss 및 정확도 계산\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # 전체 데이터에 대한 평균 loss와 정확도 계산\n",
        "    train_loss = train_loss / len(trainloader)\n",
        "    train_acc = 100 * correct / total\n",
        "\n",
        "    # 네트워크, train loss, train acuuracy 반환\n",
        "    return net, train_loss, train_acc\n",
        "\n"
      ],
      "metadata": {
        "id": "7GZp9gPxQjiN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(net, partition, criterion, args):\n",
        "    # DataLoader를 사용하여 validation data set load\n",
        "    valloader = torch.utils.data.DataLoader(partition['val'],\n",
        "                                            batch_size=args.test_batch_size,\n",
        "                                            shuffle=False, num_workers=2)\n",
        "\n",
        "    # 네트워크를 평가 모드로 설정\n",
        "    net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0\n",
        "\n",
        "    # gradient계산 비활성화 -> 평가 모드에서는 필요 없음\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            # Forward\n",
        "            outputs = net(images)\n",
        "\n",
        "            # loss 계산\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # loss 누적\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # 정확도 계산\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # 평균 loss 및 accuracy 계산\n",
        "        val_loss = val_loss / len(valloader)\n",
        "        val_acc = 100 * correct / total\n",
        "\n",
        "    # validation loss와 accuary 반환\n",
        "    return val_loss, val_acc"
      ],
      "metadata": {
        "id": "RQqHzw4OQlMk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment(partition, args):\n",
        "\n",
        "    # CNN model 지정\n",
        "    net = CNN(model_code = args.model_code,\n",
        "              in_channels = args.in_channels,\n",
        "              out_dim = args.out_dim,\n",
        "              act = args.act,\n",
        "              use_bn = args.use_bn\n",
        "              )\n",
        "\n",
        "   # GPU에 모델 할당하기\n",
        "    net.cuda()\n",
        "\n",
        "    # 손실함수 Cross-Entropy loss 설정\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer 선택\n",
        "    if args.optim == 'SGD':\n",
        "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'Adam':\n",
        "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    else:\n",
        "        raise ValueError('In-valid optimizer choice')\n",
        "\n",
        "    # train 및 validation loss/accuracy를 저장할 리스트 생성\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    for epoch in range(args.epoch):  # 데이터 셋을 여러번 반복하는 epoch loof\n",
        "        ts = time.time()\n",
        "\n",
        "        # train 함수 호출하여 모델 훈련\n",
        "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
        "\n",
        "        # validation 함수 호출하여 모델 평가\n",
        "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
        "        te = time.time()\n",
        "\n",
        "        # train, validation loss/accuracy를 리스트에 추가\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        # epoch별 train 및 validation 결과 출력\n",
        "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
        "\n",
        "    # 결과를 dictionary에 초기화 및 저장\n",
        "    result = {}\n",
        "    result['train_losses'] = train_losses\n",
        "    result['val_losses'] = val_losses\n",
        "    result['train_accs'] = train_accs\n",
        "    result['val_accs'] = val_accs\n",
        "    result['train_acc'] = train_acc\n",
        "    result['val_acc'] = val_acc\n",
        "\n",
        "    # 모델의 최종 가중치 저장\n",
        "    weight_save_path = f\"saved_weights_final_epoch_{args.epoch}.pth\"\n",
        "    torch.save(net.state_dict(), weight_save_path)\n",
        "\n",
        "    # 결과 반환\n",
        "    return vars(args), result"
      ],
      "metadata": {
        "id": "nlLtlBz2QmXm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "def save_exp_result(setting, result):\n",
        "    # setting dictionary에서 'exp_name', 'epoch', 'test_batch_size'키 삭제\n",
        "    exp_name = setting['exp_name']\n",
        "    del setting['epoch']\n",
        "    del setting['test_batch_size']\n",
        "\n",
        "    # setting 딕셔너리를 해시하여 고유한 키 생성 (앞 6자리만 사용)\n",
        "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
        "\n",
        "    # 결과 파일 경로 및 이름 설정\n",
        "    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n",
        "\n",
        "    # result 딕셔너리에 setting 정보 추가\n",
        "    result.update(setting)\n",
        "\n",
        "    # 결과를 JSON 파일로 저장\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(result, f)\n",
        ""
      ],
      "metadata": {
        "id": "ebR7C-ZAQni5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Random Seed Initialization ====== #\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "args.exp_name = \"exp1_lr\"\n",
        "\n",
        "# ====== Model Capacity ====== #\n",
        "args.model_code = 'VGG19'\n",
        "args.in_channels = 3\n",
        "args.out_dim = 100\n",
        "args.act = 'relu'\n",
        "\n",
        "# ====== Regularization ======= #\n",
        "args.l2 = 0.001\n",
        "args.use_bn = True\n",
        "\n",
        "# ====== Optimizer & Training ====== #\n",
        "args.optim = 'Adam'\n",
        "args.lr = 0.0015\n",
        "args.epoch = 30\n",
        "\n",
        "args.train_batch_size = 256\n",
        "args.test_batch_size = 1024\n",
        "\n",
        "\n",
        "setting, result = experiment(partition, args)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ026kloQo0Y",
        "outputId": "cd0fe0ee-4f59-4ba2-d679-db761012ab65"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Acc(train/val): 1.89/2.04, Loss(train/val) 4.46/4.53. Took 26.74 sec\n",
            "Epoch 1, Acc(train/val): 3.32/2.63, Loss(train/val) 4.21/4.36. Took 26.08 sec\n",
            "Epoch 2, Acc(train/val): 3.76/4.43, Loss(train/val) 4.13/4.11. Took 26.33 sec\n",
            "Epoch 3, Acc(train/val): 5.22/3.32, Loss(train/val) 4.00/4.29. Took 26.05 sec\n",
            "Epoch 4, Acc(train/val): 6.75/6.03, Loss(train/val) 3.86/4.05. Took 26.08 sec\n",
            "Epoch 5, Acc(train/val): 8.41/5.64, Loss(train/val) 3.72/4.47. Took 26.03 sec\n",
            "Epoch 6, Acc(train/val): 11.18/4.88, Loss(train/val) 3.50/4.34. Took 25.95 sec\n",
            "Epoch 7, Acc(train/val): 14.78/8.93, Loss(train/val) 3.26/3.76. Took 25.86 sec\n",
            "Epoch 8, Acc(train/val): 18.40/11.94, Loss(train/val) 3.05/3.56. Took 25.69 sec\n",
            "Epoch 9, Acc(train/val): 22.17/16.98, Loss(train/val) 2.86/3.34. Took 25.45 sec\n",
            "Epoch 10, Acc(train/val): 25.69/23.96, Loss(train/val) 2.71/2.87. Took 25.43 sec\n",
            "Epoch 11, Acc(train/val): 28.20/21.13, Loss(train/val) 2.59/3.25. Took 25.79 sec\n",
            "Epoch 12, Acc(train/val): 30.50/19.79, Loss(train/val) 2.48/3.45. Took 25.82 sec\n",
            "Epoch 13, Acc(train/val): 32.79/28.32, Loss(train/val) 2.37/2.71. Took 25.64 sec\n",
            "Epoch 14, Acc(train/val): 35.05/25.55, Loss(train/val) 2.28/3.18. Took 25.29 sec\n",
            "Epoch 15, Acc(train/val): 36.92/25.81, Loss(train/val) 2.19/3.10. Took 25.20 sec\n",
            "Epoch 16, Acc(train/val): 39.11/26.84, Loss(train/val) 2.11/2.92. Took 25.19 sec\n",
            "Epoch 17, Acc(train/val): 40.80/31.48, Loss(train/val) 2.03/2.64. Took 25.15 sec\n",
            "Epoch 18, Acc(train/val): 42.61/32.43, Loss(train/val) 1.95/2.69. Took 25.06 sec\n",
            "Epoch 19, Acc(train/val): 43.37/33.18, Loss(train/val) 1.91/2.60. Took 25.05 sec\n",
            "Epoch 20, Acc(train/val): 45.18/32.70, Loss(train/val) 1.84/2.77. Took 25.00 sec\n",
            "Epoch 21, Acc(train/val): 46.92/33.00, Loss(train/val) 1.77/2.70. Took 25.00 sec\n",
            "Epoch 22, Acc(train/val): 47.77/35.50, Loss(train/val) 1.73/2.56. Took 24.97 sec\n",
            "Epoch 23, Acc(train/val): 49.33/34.50, Loss(train/val) 1.66/2.62. Took 25.01 sec\n",
            "Epoch 24, Acc(train/val): 50.33/33.71, Loss(train/val) 1.62/2.78. Took 25.09 sec\n",
            "Epoch 25, Acc(train/val): 51.72/33.43, Loss(train/val) 1.58/2.80. Took 25.01 sec\n",
            "Epoch 26, Acc(train/val): 52.81/34.06, Loss(train/val) 1.54/2.80. Took 24.95 sec\n",
            "Epoch 27, Acc(train/val): 53.88/36.38, Loss(train/val) 1.49/2.71. Took 25.02 sec\n",
            "Epoch 28, Acc(train/val): 54.93/34.97, Loss(train/val) 1.45/2.71. Took 24.91 sec\n",
            "Epoch 29, Acc(train/val): 56.21/34.33, Loss(train/val) 1.41/2.86. Took 24.91 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_exp_result(setting, result)"
      ],
      "metadata": {
        "id": "ybde2Iw1Qp6L"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}