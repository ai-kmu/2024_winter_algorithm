{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRwOu97AlVj9",
        "outputId": "d6a85d37-8c25-4ec9-9156-abae3f0b4b91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘results’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time\n",
        "from copy import deepcopy # Add Deepcopy for args\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy"
      ],
      "metadata": {
        "id": "0g9ytA64lnit"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Subset\n",
        "import numpy as np\n",
        "\n",
        "transform = transforms.Compose(                                 # 이미지 변환을 위한 파이프라인\n",
        "    [transforms.ToTensor(),                                     # PyToch 텐서로 변환 -> 이미지를 0~1사이 값으로 만듦\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])   # 이미지를 정규화함 -> 평균, 표준편차 (0.5, 0.5, 0.5) -> 각 채널의 값이 -1에서 1 사이로 정규화\n",
        "\n",
        "# CIFAR100 train dataset 가져오기\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "# 클래스별로 인덱스를 저장할 딕셔너리 생성\n",
        "class_indices = {class_idx: [] for class_idx in range(len(trainset.classes))}\n",
        "\n",
        "# 각 데이터의 클래스 인덱스를 확인하고 해당하는 클래스 인덱스에 해당하는 리스트에 인덱스 저장\n",
        "for idx, (_, label) in enumerate(trainset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# 클래스별로 train set과 validation set의 크기 결정\n",
        "train_size_per_class = int(0.8 * len(trainset) / len(trainset.classes))\n",
        "valid_size_per_class = len(trainset) // len(trainset.classes) - train_size_per_class\n",
        "\n",
        "# train set과 validation set을 담을 리스트 초기화\n",
        "train_indices = []\n",
        "valid_indices = []\n",
        "\n",
        "# 클래스별로 데이터를 train set과 validation set으로 분할\n",
        "for class_idx, indices in class_indices.items():\n",
        "    # 클래스별로 인덱스를 랜덤하게 섞음\n",
        "    np.random.shuffle(indices)\n",
        "    # train set에 해당하는 인덱스와 validation set에 해당하는 인덱스를 각각 추가\n",
        "    train_indices.extend(indices[:train_size_per_class])\n",
        "    valid_indices.extend(indices[train_size_per_class:train_size_per_class+valid_size_per_class])\n",
        "\n",
        "# train set과 validation set을 Subset으로 생성\n",
        "train_subset = Subset(trainset, train_indices)\n",
        "valid_subset = Subset(trainset, valid_indices)\n",
        "\n",
        "# partition에 추가\n",
        "partition = {'train': train_subset, 'val': valid_subset}\n",
        "\n",
        "# 갯수 확인\n",
        "train_dataset_size = len(train_subset)\n",
        "valid_dataset_size = len(valid_subset)\n",
        "print(train_dataset_size, valid_dataset_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp9-Sfhalpei",
        "outputId": "fc88ec75-6638-48f3-f2df-db4cf702ab3b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "40000 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset가 잘 load 되었는지 체크\n",
        "sample = trainset[1][0].numpy()\n",
        "sample = numpy.transpose(sample, (1,2,0))\n",
        "height, width, _ = sample.shape\n",
        "plt.imshow(sample)\n",
        "print(height, width)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "hEgECYuWnP20",
        "outputId": "50345c75-8971-40e0-d0a8-54f7b4ef111d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32 32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backborn model 지정\n",
        "# VGGnet기반 숫자는 filter size, M은 Max pooling\n",
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}"
      ],
      "metadata": {
        "id": "3FVcjr4qnTzr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN Architecture 지정\n",
        "\n",
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self, model_code, in_channels, out_dim, act, use_bn):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # activation function 설정\n",
        "        if act == 'relu':\n",
        "            self.act = nn.ReLU()\n",
        "        elif act == 'sigmoid':\n",
        "            self.act = nn.Sigmoid()\n",
        "        elif act == 'tanh':\n",
        "            self.act = nn.TanH()\n",
        "        else:\n",
        "            raise ValueError(\"Not a valid activation function code\")\n",
        "\n",
        "        # CNN 레이어들을 생성하는 함수 호출하여 레이어 생성\n",
        "        self.layers = self._make_layers(model_code, in_channels, use_bn)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.classifer = nn.Sequential(nn.Linear(512, 256),\n",
        "                                       self.act,\n",
        "                                       self.dropout1,\n",
        "                                       nn.Linear(256, out_dim))\n",
        "    # forward 연산 정의\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)        # CNN 레이어들을 통과\n",
        "        x = x.view(x.size(0), -1) # Flatten 연산, 배치 크기는 유지하고 나머지 차원을 펼침\n",
        "        x = self.classifer(x)     # Classifier 통과\n",
        "        return x\n",
        "\n",
        "    # CNN 레이어들을 생성하는 함수\n",
        "    def _make_layers(self, model_code, in_channels, use_bn):\n",
        "        layers = []\n",
        "        for x in cfg[model_code]:\n",
        "            if x == 'M':\n",
        "                # MaxPooling 레이어 생성\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                # Convolution layer 구성\n",
        "                layers += [nn.Conv2d(in_channels=in_channels,\n",
        "                                     out_channels=x,  # out_channels은 forward 연산 이후 x\n",
        "                                     kernel_size=3,   # CNN kernel_size\n",
        "                                     stride=1,        # CNN kernel의 stride\n",
        "                                     padding=1)]      # CNN kernel의 padding\n",
        "\n",
        "                # BN(Batch Normalization을 사용하는 경우 아래를 사용)\n",
        "                if use_bn:\n",
        "                    layers += [nn.BatchNorm2d(x)]\n",
        "                layers += [self.act]   # 레이어 이후 활성화 함수 적용\n",
        "                in_channels = x        # 다음 들어가야하는 레이어에 입력 채널 수 업데이트\n",
        "\n",
        "        return nn.Sequential(*layers)  # 생성된 레이어들을 순차적으로 적용하여 모델을 정의(layer 리스트 내의 모든 레이어를 순차적으로 적용하여 하나의 시퀀스로 묶음)"
      ],
      "metadata": {
        "id": "7V-4zbErncCw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train session\n",
        "def train(net, partition, optimizer, criterion, args):\n",
        "    # trainset을 가져와서 batch_size, shuffle, num_workers(병렬적으로 데이터를 로드)\n",
        "    trainloader = torch.utils.data.DataLoader(partition['train'],\n",
        "                                              batch_size=args.train_batch_size,\n",
        "                                              shuffle=True, num_workers=2)\n",
        "    net.train() # 학습모드 설정\n",
        "\n",
        "    correct = 0 # 맞은 예측 수\n",
        "    total = 0 # 전체 데이터 수\n",
        "    train_loss = 0.0 # train_loss\n",
        "    for i, data in enumerate(trainloader, 0): # 각 배치에 대해 반복\n",
        "        optimizer.zero_grad() # 옵티마이저 그래디언트 초기화 -> 다음 step으로 가기 위함\n",
        "\n",
        "        # input(입력 데이터, 레이블)\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.cuda() # GPU 연산을 위해 GPU전송\n",
        "        labels = labels.cuda() # GPU 연산을 위해 GPU전송\n",
        "        outputs = net(inputs)  # network를 통과한 output값 (예측값)\n",
        "\n",
        "        loss = criterion(outputs, labels) # loss 계산\n",
        "        loss.backward() # backpropagaion\n",
        "        optimizer.step() # optimizer step\n",
        "\n",
        "        train_loss += loss.item() # loss를 추가\n",
        "        _, predicted = torch.max(outputs.data, 1) # output에서 가장 높은 확률값의 class 선택\n",
        "        total += labels.size(0) # 데이터 수 업데이트\n",
        "        correct += (predicted == labels).sum().item() # predict update(correct)\n",
        "\n",
        "    train_loss = train_loss / len(trainloader)  # 평균 loss\n",
        "    train_acc = 100 * correct / total # accuracy\n",
        "    return net, train_loss, train_acc"
      ],
      "metadata": {
        "id": "bxSJ7HiSn4Hc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validation seccison\n",
        "def validate(net, partition, criterion, args):\n",
        "    # validation set을 가져와서 batch_size, shuffle, num_workers(병렬적으로 데이터를 로드)\n",
        "    valloader = torch.utils.data.DataLoader(partition['val'],\n",
        "                                            batch_size=args.test_batch_size,\n",
        "                                            shuffle=False, num_workers=2)\n",
        "\n",
        "    net.eval()  # 네트워크를 평가 모드 -> backprop을 안하기를 위한 목적도 있지만 Dropout이 적용될 경우 적용하지 않기 위함도 존재\n",
        "\n",
        "    correct = 0 # 맞은 예측 수\n",
        "    total = 0 # 전체 데이터 수\n",
        "    val_loss = 0 # validation_loss\n",
        "    with torch.no_grad(): # graident X\n",
        "        for data in valloader:  # 배치마다 data 반복\n",
        "            images, labels = data\n",
        "            images = images.cuda()  # GPU 연산을 위해 GPU전송\n",
        "            labels = labels.cuda()  # GPU 연산을 위해 GPU전송\n",
        "            outputs = net(images)   # network를 통과한 output값 (예측값)\n",
        "\n",
        "            loss = criterion(outputs, labels) # loss 계산\n",
        "\n",
        "            val_loss += loss.item() # loss를 추가\n",
        "            _, predicted = torch.max(outputs.data, 1) # output에서 가장 높은 확률값의 class 선택\n",
        "            total += labels.size(0) # 데이터 수 업데이트\n",
        "            correct += (predicted == labels).sum().item() # predict update(correct)\n",
        "\n",
        "        val_loss = val_loss / len(valloader)  # 평균 loss\n",
        "        val_acc = 100 * correct / total # accuracy\n",
        "    return val_loss, val_acc"
      ],
      "metadata": {
        "id": "3YX6Pz_Sn4os"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment(partition, args):\n",
        "\n",
        "    # CNN model 지정\n",
        "    net = CNN(model_code = args.model_code,\n",
        "              in_channels = args.in_channels,\n",
        "              out_dim = args.out_dim,\n",
        "              act = args.act,\n",
        "              use_bn = args.use_bn\n",
        "              )\n",
        "    # GPU 연산을 위해 cuda로\n",
        "    net.cuda()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss() # loss 계산(class 분류)을 위한 Loss계산법 정의\n",
        "    if args.optim == 'SGD': # SGD를 사용할 때\n",
        "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'RMSprop': # RMSProp를 사용할 때\n",
        "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'Adam':  # Adam를 사용할 때\n",
        "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    else:\n",
        "        raise ValueError('In-valid optimizer choice') # 위의 optim을 사용하지 않았을때 Error송출\n",
        "\n",
        "    # 각 loss, acc에 대해 저장할 리스트 생성\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "\n",
        "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
        "        ts = time.time()  # 학습시간을 계산하기 위해 초기 시간 기록\n",
        "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args) # 위에서 정의 한 train function 호출\n",
        "        val_loss, val_acc = validate(net, partition, criterion, args) # 위에서 정의한 validation\n",
        "        te = time.time() # 종료 시간 -> 한번 학습이 돌아갔을때의 시간 (te - ts -> 학습시간)\n",
        "\n",
        "        # 각 loss를 저장함\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        # 한 epoch당 acc, loss를 뱉어줌\n",
        "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
        "\n",
        "        # 매 에폭이 끝날 때마다 모델의 가중치를 저장\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            weight_save_path = f\"saved_weights_epoch_{epoch + 1}.pth\"  # 에폭에 따른 파일명 생성\n",
        "            torch.save(net.state_dict(), weight_save_path)\n",
        "\n",
        "    # test를 할 떄 불러옴\n",
        "    #test_acc = test(net, partition, args)\n",
        "\n",
        "    # 결과를 result에 따로 저장함\n",
        "    result = {}\n",
        "    result['train_losses'] = train_losses\n",
        "    result['val_losses'] = val_losses\n",
        "    result['train_accs'] = train_accs\n",
        "    result['val_accs'] = val_accs\n",
        "    result['train_acc'] = train_acc\n",
        "    result['val_acc'] = val_acc\n",
        "    #result['test_acc'] = test_acc\n",
        "    return vars(args), result"
      ],
      "metadata": {
        "id": "wpl8Votsn6fo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "def save_exp_result(setting, result):\n",
        "    exp_name = setting['exp_name']  # exp_name을 가져옴\n",
        "    del setting['epoch']  # epoch key delete\n",
        "    del setting['test_batch_size']  # test_batch delete\n",
        "\n",
        "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]  # 설정을 해싱\n",
        "    filename = './results/{}-{}.json'.format(exp_name, hash_key)  # 결과를 저장할 파일 경로 지정\n",
        "    result.update(setting)  # 설정을 result에 update\n",
        "    with open(filename, 'w') as f:  # exp_name, hash_key를 가지는 파일 이름으로 저장\n",
        "        json.dump(result, f)"
      ],
      "metadata": {
        "id": "Sd1-sjPfoABU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Random Seed Initialization ====== #\n",
        "# 동일한 코드를 여러번 실행할 때 동일한 결과를 얻을 수 있는데 이를 방지하기 위함\n",
        "seed = 123\n",
        "np.random.seed(seed)  # numpy random seed\n",
        "torch.manual_seed(seed) # torch random seed\n",
        "\n",
        "parser = argparse.ArgumentParser()  # 명령행 옵션을 파싱\n",
        "args = parser.parse_args(\"\")  # 파싱 이후 결과 저장\n",
        "args.exp_name = \"exp1_lr_model_code\"  # exp_name 저장\n",
        "\n",
        "# ====== Model ====== #\n",
        "args.model_code = 'VGG11'\n",
        "args.in_channels = 3  # input channel\n",
        "args.out_dim = 100   # output channel\n",
        "args.act = 'relu'   # activation function\n",
        "\n",
        "# ====== Regularization ======= #\n",
        "args.l2 = 0.01   # L2 Norm\n",
        "args.use_bn = True  # Batch Normalization\n",
        "\n",
        "# ====== Optimizer & Training ====== #\n",
        "args.optim = 'Adam' #'RMSprop' #SGD, RMSprop, ADAM...\n",
        "args.lr = 0.00005  # learning rate\n",
        "args.epoch = 100   # epoch\n",
        "\n",
        "args.train_batch_size = 512 # train_batch_size\n",
        "args.test_batch_size = 1024 # test_batch_size\n",
        "\n",
        "setting, result = experiment(partition, args)\n",
        "save_exp_result(setting, result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PdwHqG1OoDiX",
        "outputId": "b6d4a46d-9575-4085-88e2-341669f21dad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Acc(train/val): 7.30/13.97, Loss(train/val) 4.29/3.95. Took 14.92 sec\n",
            "Epoch 1, Acc(train/val): 16.64/23.47, Loss(train/val) 3.72/3.46. Took 14.13 sec\n",
            "Epoch 2, Acc(train/val): 24.07/28.56, Loss(train/val) 3.28/3.12. Took 14.26 sec\n",
            "Epoch 3, Acc(train/val): 31.36/32.01, Loss(train/val) 2.90/2.87. Took 14.91 sec\n",
            "Epoch 4, Acc(train/val): 38.99/34.20, Loss(train/val) 2.54/2.77. Took 18.23 sec\n",
            "Epoch 5, Acc(train/val): 47.12/36.09, Loss(train/val) 2.19/2.65. Took 16.76 sec\n",
            "Epoch 6, Acc(train/val): 56.84/36.94, Loss(train/val) 1.82/2.59. Took 14.37 sec\n",
            "Epoch 7, Acc(train/val): 67.19/36.90, Loss(train/val) 1.45/2.58. Took 14.50 sec\n",
            "Epoch 8, Acc(train/val): 77.77/36.92, Loss(train/val) 1.08/2.61. Took 14.80 sec\n",
            "Epoch 9, Acc(train/val): 86.91/37.25, Loss(train/val) 0.75/2.58. Took 14.12 sec\n",
            "Epoch 10, Acc(train/val): 93.33/36.56, Loss(train/val) 0.49/2.67. Took 14.75 sec\n",
            "Epoch 11, Acc(train/val): 96.93/37.05, Loss(train/val) 0.32/2.65. Took 14.57 sec\n",
            "Epoch 12, Acc(train/val): 98.64/36.64, Loss(train/val) 0.21/2.68. Took 14.88 sec\n",
            "Epoch 13, Acc(train/val): 99.47/36.99, Loss(train/val) 0.15/2.70. Took 14.50 sec\n",
            "Epoch 14, Acc(train/val): 99.70/37.77, Loss(train/val) 0.12/2.67. Took 14.80 sec\n",
            "Epoch 15, Acc(train/val): 99.84/37.65, Loss(train/val) 0.09/2.69. Took 14.45 sec\n",
            "Epoch 16, Acc(train/val): 99.92/37.62, Loss(train/val) 0.08/2.69. Took 14.18 sec\n",
            "Epoch 17, Acc(train/val): 99.91/37.57, Loss(train/val) 0.08/2.71. Took 14.55 sec\n",
            "Epoch 18, Acc(train/val): 99.89/37.97, Loss(train/val) 0.06/2.70. Took 14.27 sec\n",
            "Epoch 19, Acc(train/val): 99.96/37.81, Loss(train/val) 0.05/2.71. Took 14.21 sec\n",
            "Epoch 20, Acc(train/val): 99.97/38.10, Loss(train/val) 0.05/2.70. Took 14.61 sec\n",
            "Epoch 21, Acc(train/val): 99.96/37.51, Loss(train/val) 0.05/2.72. Took 14.99 sec\n",
            "Epoch 22, Acc(train/val): 99.96/36.98, Loss(train/val) 0.05/2.72. Took 14.24 sec\n",
            "Epoch 23, Acc(train/val): 99.94/36.15, Loss(train/val) 0.05/2.80. Took 14.35 sec\n",
            "Epoch 24, Acc(train/val): 99.96/36.96, Loss(train/val) 0.05/2.75. Took 14.32 sec\n",
            "Epoch 25, Acc(train/val): 99.96/36.14, Loss(train/val) 0.05/2.78. Took 14.30 sec\n",
            "Epoch 26, Acc(train/val): 99.96/36.02, Loss(train/val) 0.05/2.80. Took 14.38 sec\n",
            "Epoch 27, Acc(train/val): 99.97/36.01, Loss(train/val) 0.05/2.82. Took 14.45 sec\n",
            "Epoch 28, Acc(train/val): 99.96/36.42, Loss(train/val) 0.05/2.80. Took 15.17 sec\n",
            "Epoch 29, Acc(train/val): 99.96/35.61, Loss(train/val) 0.05/2.82. Took 14.75 sec\n",
            "Epoch 30, Acc(train/val): 99.98/36.81, Loss(train/val) 0.05/2.76. Took 14.52 sec\n",
            "Epoch 31, Acc(train/val): 99.98/35.12, Loss(train/val) 0.04/2.87. Took 14.63 sec\n",
            "Epoch 32, Acc(train/val): 99.96/34.99, Loss(train/val) 0.05/2.86. Took 14.34 sec\n",
            "Epoch 33, Acc(train/val): 99.97/32.67, Loss(train/val) 0.05/3.01. Took 14.43 sec\n",
            "Epoch 34, Acc(train/val): 99.22/23.75, Loss(train/val) 0.16/3.78. Took 15.27 sec\n",
            "Epoch 35, Acc(train/val): 78.68/23.73, Loss(train/val) 0.92/3.58. Took 14.98 sec\n",
            "Epoch 36, Acc(train/val): 93.21/32.47, Loss(train/val) 0.37/3.09. Took 14.44 sec\n",
            "Epoch 37, Acc(train/val): 99.22/36.84, Loss(train/val) 0.10/2.83. Took 14.43 sec\n",
            "Epoch 38, Acc(train/val): 99.92/37.91, Loss(train/val) 0.03/2.75. Took 14.52 sec\n",
            "Epoch 39, Acc(train/val): 99.97/38.56, Loss(train/val) 0.03/2.71. Took 14.50 sec\n",
            "Epoch 40, Acc(train/val): 99.98/38.50, Loss(train/val) 0.03/2.67. Took 14.74 sec\n",
            "Epoch 41, Acc(train/val): 99.98/38.83, Loss(train/val) 0.03/2.66. Took 15.16 sec\n",
            "Epoch 42, Acc(train/val): 99.97/38.90, Loss(train/val) 0.03/2.65. Took 14.27 sec\n",
            "Epoch 43, Acc(train/val): 99.98/38.75, Loss(train/val) 0.03/2.66. Took 14.59 sec\n",
            "Epoch 44, Acc(train/val): 99.99/38.72, Loss(train/val) 0.03/2.68. Took 14.54 sec\n",
            "Epoch 45, Acc(train/val): 99.98/38.92, Loss(train/val) 0.03/2.66. Took 14.35 sec\n",
            "Epoch 46, Acc(train/val): 99.98/38.66, Loss(train/val) 0.04/2.69. Took 14.79 sec\n",
            "Epoch 47, Acc(train/val): 99.98/38.59, Loss(train/val) 0.04/2.68. Took 15.62 sec\n",
            "Epoch 48, Acc(train/val): 99.98/38.53, Loss(train/val) 0.04/2.70. Took 14.28 sec\n",
            "Epoch 49, Acc(train/val): 99.98/38.32, Loss(train/val) 0.04/2.71. Took 14.51 sec\n",
            "Epoch 50, Acc(train/val): 99.97/37.94, Loss(train/val) 0.04/2.72. Took 14.72 sec\n",
            "Epoch 51, Acc(train/val): 99.98/38.13, Loss(train/val) 0.04/2.71. Took 14.39 sec\n",
            "Epoch 52, Acc(train/val): 99.98/37.71, Loss(train/val) 0.04/2.74. Took 14.55 sec\n",
            "Epoch 53, Acc(train/val): 99.98/36.57, Loss(train/val) 0.04/2.82. Took 14.87 sec\n",
            "Epoch 54, Acc(train/val): 99.99/37.96, Loss(train/val) 0.04/2.74. Took 14.77 sec\n",
            "Epoch 55, Acc(train/val): 99.98/34.30, Loss(train/val) 0.04/2.93. Took 14.41 sec\n",
            "Epoch 56, Acc(train/val): 99.97/36.04, Loss(train/val) 0.05/2.81. Took 14.47 sec\n",
            "Epoch 57, Acc(train/val): 99.95/35.35, Loss(train/val) 0.05/2.86. Took 14.15 sec\n",
            "Epoch 58, Acc(train/val): 99.85/27.49, Loss(train/val) 0.08/3.47. Took 14.24 sec\n",
            "Epoch 59, Acc(train/val): 84.14/22.94, Loss(train/val) 0.76/3.57. Took 14.36 sec\n",
            "Epoch 60, Acc(train/val): 81.52/27.55, Loss(train/val) 0.77/3.27. Took 14.45 sec\n",
            "Epoch 61, Acc(train/val): 96.84/33.17, Loss(train/val) 0.21/3.11. Took 14.51 sec\n",
            "Epoch 62, Acc(train/val): 99.69/36.96, Loss(train/val) 0.06/2.82. Took 14.07 sec\n",
            "Epoch 63, Acc(train/val): 99.94/38.44, Loss(train/val) 0.03/2.73. Took 14.05 sec\n",
            "Epoch 64, Acc(train/val): 99.97/38.94, Loss(train/val) 0.02/2.70. Took 14.31 sec\n",
            "Epoch 65, Acc(train/val): 99.97/38.95, Loss(train/val) 0.02/2.67. Took 14.24 sec\n",
            "Epoch 66, Acc(train/val): 99.97/39.27, Loss(train/val) 0.03/2.66. Took 14.11 sec\n",
            "Epoch 67, Acc(train/val): 99.97/38.77, Loss(train/val) 0.03/2.67. Took 14.55 sec\n",
            "Epoch 68, Acc(train/val): 99.97/38.85, Loss(train/val) 0.03/2.67. Took 15.05 sec\n",
            "Epoch 69, Acc(train/val): 99.98/39.19, Loss(train/val) 0.03/2.66. Took 14.42 sec\n",
            "Epoch 70, Acc(train/val): 99.98/38.65, Loss(train/val) 0.04/2.68. Took 13.83 sec\n",
            "Epoch 71, Acc(train/val): 99.97/38.77, Loss(train/val) 0.04/2.67. Took 14.07 sec\n",
            "Epoch 72, Acc(train/val): 99.98/39.07, Loss(train/val) 0.04/2.69. Took 14.44 sec\n",
            "Epoch 73, Acc(train/val): 99.98/38.88, Loss(train/val) 0.04/2.68. Took 14.06 sec\n",
            "Epoch 74, Acc(train/val): 99.98/39.17, Loss(train/val) 0.04/2.69. Took 13.82 sec\n",
            "Epoch 75, Acc(train/val): 99.98/38.32, Loss(train/val) 0.04/2.72. Took 14.14 sec\n",
            "Epoch 76, Acc(train/val): 99.98/38.60, Loss(train/val) 0.04/2.72. Took 14.65 sec\n",
            "Epoch 77, Acc(train/val): 99.99/38.41, Loss(train/val) 0.04/2.72. Took 14.26 sec\n",
            "Epoch 78, Acc(train/val): 99.98/38.08, Loss(train/val) 0.04/2.75. Took 13.85 sec\n",
            "Epoch 79, Acc(train/val): 99.98/38.09, Loss(train/val) 0.04/2.74. Took 13.88 sec\n",
            "Epoch 80, Acc(train/val): 99.98/37.99, Loss(train/val) 0.04/2.75. Took 13.87 sec\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-51122b36d57b>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;31m# test_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0msetting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0msave_exp_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-18510e8f8e61>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(partition, args)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# loop over the dataset multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 학습시간을 계산하기 위해 초기 시간 기록\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 위에서 정의 한 train function 호출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 위에서 정의한 validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 종료 시간 -> 한번 학습이 돌아갔을때의 시간 (te - ts -> 학습시간)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-9ac39cee1823>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, partition, optimizer, criterion, args)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# optimizer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# loss를 추가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# output에서 가장 높은 확률값의 class 선택\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 데이터 수 업데이트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}